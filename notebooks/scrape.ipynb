{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A very simple and basic web scraping script. Feel free to\n",
    "use this as a source of inspiration, but, make sure to attribute\n",
    "it if you do so.\n",
    "\n",
    "This is by no means production code.\n",
    "\"\"\"\n",
    "# built-in imports\n",
    "import re\n",
    "import pandas as pd\n",
    "from json import dump\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# user packages\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "\n",
    "# constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "N_PAGES = range(1, 2) # update this to your liking\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Web Scraping</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3000\n",
       "1      3001\n",
       "2      3002\n",
       "3      3003\n",
       "4      3004\n",
       "       ... \n",
       "711    3990\n",
       "712    3991\n",
       "713    3992\n",
       "714    3995\n",
       "715    3996\n",
       "Name: 0, Length: 716, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postcodes = pd.read_csv(\"../data/curated/unique_postcodes.csv\", header=None).squeeze()\n",
    "postcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Progress tracker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports & functions for progress tracking, can be removed for final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output \n",
    "import timeit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_tracker(start, stop, curr_progress):\n",
    "    if (curr_progress*100) < 5:\n",
    "        expected_time = \"Calculating...\"\n",
    "    else:\n",
    "        time_perc = timeit.default_timer()\n",
    "        expected_time = np.round(((time_perc-start) / curr_progress)/60, 2)\n",
    "\n",
    "    print(\"Current progress:\", np.round(curr_progress*100, 2), \"%\")\n",
    "    print(\"current run time:\", np.round((stop-start)/60, 2), \"minutes\")\n",
    "    print(\"Expected run time:\", expected_time, \"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selenium (optional running)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "driver_location = \"/usr/bin/chromedriver\"\n",
    "binary_location = \"/usr/bin/google-chrome\"\n",
    "\n",
    "options = Options()\n",
    "options.binary_location = binary_location\n",
    "\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "s=Service(driver_location)\n",
    "\n",
    "driver = webdriver.Chrome(service=s, options=options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URL Scraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current progress: 99.86 %\n",
      "current run time: 5.49 minutes\n",
      "Expected run time: 5.5 minutes\n"
     ]
    }
   ],
   "source": [
    "# begin code\n",
    "url_dict = defaultdict(dict)\n",
    "property_metadata = defaultdict(dict)\n",
    "\n",
    "start = timeit.default_timer() # for progress tracking\n",
    "\n",
    "# index & enumerate are for progress tracking, can be removed for final submission\n",
    "for index, postcode in enumerate(postcodes):\n",
    "    clear_output(wait=True) # for progress tracking\n",
    "\n",
    "    url_links = []\n",
    "    \n",
    "    # generate list of urls to visit\n",
    "    for page in N_PAGES:\n",
    "        # need to decide regions to analyse         \n",
    "        url = BASE_URL + f\"/rent/?postcode={postcode}&page={page}\" # a single page\n",
    "        bs_object = BeautifulSoup(requests.get(url, headers=headers).text, \"html.parser\") # makes bs object\n",
    "\n",
    "        # find the unordered list (ul) elements which are the results, then\n",
    "        # find all href (a) tags that are from the base_url website.\n",
    "        index_links = bs_object.find(\"ul\", {\"data-testid\": \"results\"}) \n",
    "\n",
    "        # if there are no links, break\n",
    "        if (index_links == None):\n",
    "            break \n",
    "\n",
    "        index_links = index_links.findAll(\"a\",href=re.compile(f\"{BASE_URL}/*\")) # complies RE string into RE expression\n",
    "            \n",
    "        for link in index_links:\n",
    "            # if its a property address, add it to the list\n",
    "            if 'address' in link['class']:\n",
    "                url_links.append(link['href'])\n",
    "\n",
    "    url_dict[postcode] = url_links\n",
    "\n",
    "    # for progress tracking\n",
    "    curr_progress = index/len(postcodes)\n",
    "    stop = timeit.default_timer()\n",
    "    progress_tracker(start, stop, curr_progress)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Scraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/Jade/Documents/GitHub/generic-real-estate-consulting-project-group-42/notebooks/scrape.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/Jade/Documents/GitHub/generic-real-estate-consulting-project-group-42/notebooks/scrape.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m url_links \u001b[39m=\u001b[39m url_dict\u001b[39m.\u001b[39mget(postcode)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/Jade/Documents/GitHub/generic-real-estate-consulting-project-group-42/notebooks/scrape.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m property_url \u001b[39min\u001b[39;00m url_links:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/Jade/Documents/GitHub/generic-real-estate-consulting-project-group-42/notebooks/scrape.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     bs_object \u001b[39m=\u001b[39m BeautifulSoup(requests\u001b[39m.\u001b[39;49mget(property_url, headers\u001b[39m=\u001b[39;49mheaders)\u001b[39m.\u001b[39;49mtext, \u001b[39m\"\u001b[39;49m\u001b[39mhtml.parser\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/Jade/Documents/GitHub/generic-real-estate-consulting-project-group-42/notebooks/scrape.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# looks for the header class to get property name\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/Jade/Documents/GitHub/generic-real-estate-consulting-project-group-42/notebooks/scrape.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     property_metadata[property_url][\u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m bs_object \\\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/Jade/Documents/GitHub/generic-real-estate-consulting-project-group-42/notebooks/scrape.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mh1\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcss-164r41r\u001b[39m\u001b[39m\"\u001b[39m}) \\\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/Jade/Documents/GitHub/generic-real-estate-consulting-project-group-42/notebooks/scrape.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39m.\u001b[39mtext\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/__init__.py:333\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39minitialize_soup(\u001b[39mself\u001b[39m)\n\u001b[1;32m    332\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feed()\n\u001b[1;32m    334\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/__init__.py:451\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39m# Convert the document to Unicode.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 451\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mfeed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmarkup)\n\u001b[1;32m    452\u001b[0m \u001b[39m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendData()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/builder/_htmlparser.py:399\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    397\u001b[0m parser\u001b[39m.\u001b[39msoup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\n\u001b[1;32m    398\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     parser\u001b[39m.\u001b[39;49mfeed(markup)\n\u001b[1;32m    400\u001b[0m     parser\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    401\u001b[0m \u001b[39mexcept\u001b[39;00m HTMLParseError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.8/html/parser.py:111\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[39mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39mas you want (may include '\\n').\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m+\u001b[39m data\n\u001b[0;32m--> 111\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgoahead(\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/html/parser.py:171\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m startswith(\u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m, i):\n\u001b[1;32m    170\u001b[0m     \u001b[39mif\u001b[39;00m starttagopen\u001b[39m.\u001b[39mmatch(rawdata, i): \u001b[39m# < + letter\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m         k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_starttag(i)\n\u001b[1;32m    172\u001b[0m     \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m</\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[1;32m    173\u001b[0m         k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_endtag(i)\n",
      "File \u001b[0;32m/usr/lib/python3.8/html/parser.py:345\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_startendtag(tag, attrs)\n\u001b[1;32m    344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_starttag(tag, attrs)\n\u001b[1;32m    346\u001b[0m     \u001b[39mif\u001b[39;00m tag \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCDATA_CONTENT_ELEMENTS:\n\u001b[1;32m    347\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_cdata_mode(tag)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/builder/_htmlparser.py:154\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_starttag\u001b[0;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39m#print(\"START\", name)\u001b[39;00m\n\u001b[1;32m    153\u001b[0m sourceline, sourcepos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetpos()\n\u001b[0;32m--> 154\u001b[0m tag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msoup\u001b[39m.\u001b[39;49mhandle_starttag(\n\u001b[1;32m    155\u001b[0m     name, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, attr_dict, sourceline\u001b[39m=\u001b[39;49msourceline,\n\u001b[1;32m    156\u001b[0m     sourcepos\u001b[39m=\u001b[39;49msourcepos\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m tag \u001b[39mand\u001b[39;00m tag\u001b[39m.\u001b[39mis_empty_element \u001b[39mand\u001b[39;00m handle_empty_element:\n\u001b[1;32m    159\u001b[0m     \u001b[39m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[39m# events for empty-element tags. (It's handled in\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# don't want handle_endtag() to cross off any previous end\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# events for tags of this name.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_endtag(name, check_already_closed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/__init__.py:721\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_starttag\u001b[0;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos, namespaces)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_only \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagStack) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    717\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_only\u001b[39m.\u001b[39mtext\n\u001b[1;32m    718\u001b[0m          \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_only\u001b[39m.\u001b[39msearch_tag(name, attrs))):\n\u001b[1;32m    719\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m tag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49melement_classes\u001b[39m.\u001b[39;49mget(Tag, Tag)(\n\u001b[1;32m    722\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder, name, namespace, nsprefix, attrs,\n\u001b[1;32m    723\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrentTag, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_most_recent_element,\n\u001b[1;32m    724\u001b[0m     sourceline\u001b[39m=\u001b[39;49msourceline, sourcepos\u001b[39m=\u001b[39;49msourcepos,\n\u001b[1;32m    725\u001b[0m     namespaces\u001b[39m=\u001b[39;49mnamespaces\n\u001b[1;32m    726\u001b[0m )\n\u001b[1;32m    727\u001b[0m \u001b[39mif\u001b[39;00m tag \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    728\u001b[0m     \u001b[39mreturn\u001b[39;00m tag\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/element.py:1290\u001b[0m, in \u001b[0;36mTag.__init__\u001b[0;34m(self, parser, builder, name, namespace, prefix, attrs, parent, previous, is_xml, sourceline, sourcepos, can_be_empty_element, cdata_list_attributes, preserve_whitespace_tags, interesting_string_types, namespaces)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[39m# Keep track of the names that might cause this tag to be treated as a\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[39m# whitespace-preserved tag.\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreserve_whitespace_tags \u001b[39m=\u001b[39m builder\u001b[39m.\u001b[39mpreserve_whitespace_tags\n\u001b[0;32m-> 1290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m builder\u001b[39m.\u001b[39;49mstring_containers:\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# This sort of tag uses a special string container\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[39m# subclass for most of its strings. When we ask the\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minteresting_string_types \u001b[39m=\u001b[39m builder\u001b[39m.\u001b[39mstring_containers[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname]\n\u001b[1;32m   1294\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer() # for progress tracking\n",
    "\n",
    "# for each url, scrape features\n",
    "# index & enumerate are for progress tracking, can be removed for final submission\n",
    "for index, postcode in enumerate(url_dict):\n",
    "    clear_output(wait=True) # for progress tracking\n",
    "\n",
    "    url_links = url_dict.get(postcode)\n",
    "\n",
    "    for property_url in url_links:\n",
    "        bs_object = BeautifulSoup(requests.get(property_url, headers=headers).text, \"html.parser\")\n",
    "\n",
    "        # looks for the header class to get property name\n",
    "        property_metadata[property_url]['Name'] = bs_object \\\n",
    "            .find(\"h1\", {\"class\": \"css-164r41r\"}) \\\n",
    "            .text\n",
    "\n",
    "        # regex to find the cost in the summary title \n",
    "        cost_finder = re.compile(r'[0-9]+.?[0-9]+') # this regex search assumes that the first numeric value is the cost per week \n",
    "        # looks for the div containing a summary title for cost\n",
    "        cost_text = bs_object \\\n",
    "            .find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}) \\\n",
    "            .text\n",
    "\n",
    "        # extracts the cost from the summary title and adds to dictionary. \n",
    "        # if there is no cost written in the summary title, it is replaced by 0 \n",
    "        cost = cost_finder.search(cost_text)\n",
    "        if cost == None: \n",
    "            property_metadata[property_url]['Cost'] = 0\n",
    "        else:\n",
    "            property_metadata[property_url]['Cost'] = cost.group()\n",
    "            \n",
    "        # extract coordinates from the hyperlink provided\n",
    "        # finds latitude and longitude from integrated Google Map\n",
    "        property_metadata[property_url]['Coordinates'] = [\n",
    "            float(coord) for coord in re.findall(\n",
    "                r'destination=([-\\s,\\d\\.]+)', # use regex101.com here if you need to\n",
    "                bs_object \\\n",
    "                    .find(\n",
    "                        \"a\",\n",
    "                        {\"target\": \"_blank\", 'rel': \"noopener noreferer\"}\n",
    "                    ) \\\n",
    "                    .attrs['href']\n",
    "            )[0].split(',')\n",
    "        ]\n",
    "        \n",
    "        # extracts # of bedrooms, # of baths and # of parking spots \n",
    "        rooms_info = bs_object.find(\"div\", {\"data-testid\": \"property-features\"}).findAll(\"span\", {\"data-testid\": \"property-features-text-container\"})\n",
    "        for i in range(0, len(rooms_info)):\n",
    "            attr_desc = str(rooms_info[i].text).split(' ')\n",
    "\n",
    "            property_metadata[property_url][attr_desc[1]] = attr_desc[0]\n",
    "\n",
    "        # extracts property type from the site \n",
    "        property_metadata[property_url]['Property_Type'] = bs_object \\\n",
    "            .find(\"div\", {\"data-testid\": \"listing-summary-property-type\"}).find(\"span\", {\"class\" : \"css-in3yi3\"}).text\n",
    "\n",
    "        # extracts desciption from the site \n",
    "        # will significantly increase run-time\n",
    "        '''\n",
    "        driver.get(property_url)\n",
    "        read_more_button = driver.find_element(by=By.CSS_SELECTOR , value='[data-testid=\"listing-details__description-button\"]')\n",
    "        read_more_button.click()\n",
    "        property_metadata[property_url]['Desc'] = driver.find_element(by=By.CSS_SELECTOR, value='[data-testid=\"listing-details__description\"]').text\n",
    "        '''\n",
    "\n",
    "        # extract real estate agency\n",
    "        property_metadata[property_url]['Agency'] = bs_object.find(\"a\", {\"data-testid\" : \"listing-details__agent-details-agent-company-name\"}).text\n",
    "\n",
    "        # add postcode\n",
    "        property_metadata[property_url]['Postcode'] = postcode\n",
    "\n",
    "    # for progress tracking\n",
    "    curr_progress = index/len(url_dict)\n",
    "    stop = timeit.default_timer()\n",
    "    progress_tracker(start, stop, curr_progress)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "property_df = pd.DataFrame(columns=['Name', 'Cost', 'Coordinates', 'Bed', 'Bath', 'Parking', 'Agency', 'Postcode'])\n",
    "\n",
    "for index, postcode in enumerate(postcodes):\n",
    "    for url in url_dict.get(postcode):\n",
    "        info = property_metadata.get(url)\n",
    "\n",
    "        if info == None:\n",
    "            break\n",
    "        else:\n",
    "            data.append(list(info.values()))\n",
    "        \n",
    "\n",
    "\n",
    "property_df = pd.DataFrame(data)\n",
    "property_df.to_csv('../data/raw/property_data.csv')\n",
    "       \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
